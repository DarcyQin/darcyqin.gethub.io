---
layout: post
title:  "EM算法"
date:   2019-06-20 23:30
categories: algorithm
tags: algorithm, ML
excerpt: 
mathjax: true
---

* content
{:toc}


## 凸函数
开始EM算法前，我们先讨论凸函数。连续函数的一阶导数就是相应的切线斜率。一阶导数大于0，则递增；一阶倒数小于0，则递减；一阶导数等于0，则不增不减。 而二阶导数可以反映图象的凹凸。二阶导数大于0，图象为凹；二阶导数小于0，图象为凸；二阶导数等于0，不凹不凸。

现设一函数f(x)，在该函数定义域的凸区间内任取两点$x_1$,$x_2$, 其中$x_1<x_2$,设一点$x=q_1x_1+q_2x_2$,($q_1$,$q_2>0$且$q_1+q_2=1$),那么该点必包含于$x_1$,$x_2$之间。

![image03](https://darcyqin.github.io/img/tufunc.png)

图中包含点$A_1(x_1,x_2)$、$A_2(x_2,x_2)$,$A(x,f(x))$,先构建线性函数两点式，代入$A_1(x_1,x_2)$、$A_2(x_2,x_2)$两点，再代入B点的横坐标，得到其纵坐标：

$$y=\frac{x_2-x}{x_2-x_1}y_1+\frac{x-x_1}{x_2-x_1}y_2$$

同时利用$x=q_1x_1+q_2x_2$可以得：


$$q_1=\frac{x_2-x}{x_2-x_1}$$

$$q_2=\frac{x-x_1}{x_2-x_1}$$

所以B的纵坐标是：

$$y=q_1f(x_1)+q_2f(x_2)$$

结合上图，显然：

$$y_B≥y_A$$

$$=> q_1f(x_1)+q_2f(x_2)≥f(q_1x_1+q_2x_2)$$

且当$x_1 -> x_2$时，可以等号成立。

总结下：   
在区间X上，有定义而且连续的函数$f(x)$叫做凸函数（往下凸），如果对于区间x中的任何两点$x_1$和$x_2(x_2>=x_1)$有不等式：

$$f(q_1x_1+q_2x_2) ≤q_1f(x_1)+q_2f(x_2)$$

同理，当函数为凹函数是：

$$f(q_1x_1+q_2x_2) ≥q_1f(x_1)+q_2f(x_2)$$


## Jensen不等式
刚刚我们已经证明的凸函数的代数定义：

$$q_1f(x_1)+q_2f(x_2)≥f(q_1x_1+q_2x_2)$$

将其一般化，便容易推出：

$$f(q_1x_1+q_2x_2+...+q_nx_n)≤q_1f(x_1)+q_2f(x_2)+...+q_nf(x_n)$$

$$(q_1,...,q_n>0,q_1+...+q_n=1)$$

将形式更加简化得：

$$f(\sum_{i=1}^np_ix_i)≤\sum_{i=1}^np_if(x_i)$$

在概率论中，如果把$p_i$看成是取值为$x_i$的离散变量x的概率分布，那么上式可以写成：

$$f(E(x))≤E(f(x))$$

其中$E(.)$表示期望。

## EM算法
EM算法是一种迭代算法，主要用来处理含有隐变量（hidden variable）的概率参数的极大似然估计，或者极大后验概率估计。EM算法的每次迭代分为两步：E步，求期望；M步，求极大。所以该算法称为期望极大算法（expectation maximization algorithm），简称EM算法。
### EM算法公式推导
推导过程参照李航<统计学习方法>，加上个人理解。

当我们面对一个含有隐变量的概率模型时，我们的最终目标是要极大化观测数据$Y$关于参数$\theta$的对数似然函数，即极大化：

$$L(\theta)=logP(Y|\theta)$$

将所有隐变量于观测数据$Y$的所有联合概率相加，即：

$$P(Y|\theta)=\sum_ZP(Y,Z|\theta)$$


$$L(\theta)=\log\sum_ZP(Y,Z|\theta)=\log\left(\sum_ZP(Y|Z,\theta)P(Z|\theta)\right)$$

注意到要极大化上式非常困难的，式中不仅有未观测数据$Z$,还包含了求和的对数，对未知变量求导后等于0的式子仍然十分复杂。
我们可以通过迭代的方式不断的逼近极大化的$L(\theta)$，假设在第i次的迭代后的$\theta$的估计值是$theta^{(i)}$,如果能够使新估计的$\theta$，让$L(\theta)$增加，即，使得$L(\theta)>L(\theta^{(i)})$,那么在一次次的迭代之后，最终会逼近$L(\theta)$的极大值。
即：
	
$$L(\theta)-L(\theta^{(i)})=\log\left(\sum_ZP(Y|Z,\theta)P(Z|\theta)\right)-logP(Y|\theta^{(i)})$$

利用上面的Jensen不等式，得到其下界：

$$
\begin{align}	
L(\theta)-L(\theta^{(i)})&=\log\left(\sum_ZP(Y|Z,\theta)P(Z|\theta)\right)-logP(Y|\theta^{(i)})\\

&=\log\left(\sum_ZP(Z|Y,\theta^{(i)})\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}\right)-logP(Y|\theta^{(i)})\\

&≥\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}-logP(Y|\theta^{(i)})
\end{align}
$$

因为$P(Z|Y,\theta^{(i)})$是Z关于Y的概率分布，所以:
$$\sum_ZP(Z|Y,\theta^{(i)})=1$$
即上式：

$$≥\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})}-\sum_ZP(Z|Y,\theta^{(i)})logP(Y|\theta^{(i)})$$

$$=\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}$$

令：

$$B(\theta,\theta^{(i)})=L(\theta^{(i)})+\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}$$

则：$L(\theta)≥B(\theta,\theta^{(i)})$,即函数$B(\theta,\theta^{(i)})$是$L(\theta)$的一个下界，由上面的式子可知：

$$L(\theta^{(i)})=B(\theta^{(i)},\theta^{(i)})$$

因此对于任何能使$B(\theta,\theta^{(i)})$增大的$\theta$，也同时可以使$L(\theta)$增大，为了使$L(\theta)$增大，选择$\theta^{(i)}$使得$B(\theta,\theta^{(i)})$达到极大，即：

$$\theta^{(i+1)}=arg\max_{\theta}B(\theta,\theta^{(i)})$$

去掉不相关的常数：

$$
\begin{align}
\theta^{(i+1)}&=arg\max_{\theta}\left(L(\theta^{(i)})+\sum_ZP(Z|Y,\theta^{(i)})\log\frac{P(Y|Z,\theta)P(Z|\theta)}{P(Z|Y,\theta^{(i)})P(Y|\theta^{(i)})}\right)\\

&=arg\max_{\theta}\left(\sum_ZP(Z|Y,\theta^{(i)})\log(P(Y|Z,\theta)P(Z|\theta))\right)\\

&=arg\max_{\theta}\left(\sum_ZP(Z|Y,\theta^{(i)})logP(Y,Z|\theta)\right)\\

&=arg\max_{\theta}Q(\theta,\theta^{(i)})
\end{align}
$$

### EM算法的收敛性证明

由于：
	
$$P(Y|\theta)=\frac{P(Y,Z|\theta)}{P(Z|Y,\theta)}$$

两边取对数有：

$$logP(Y|\theta)=logP(Y,Z|\theta)-logP(Z|Y,\theta)$$

等式两边同时对分布
$$P(Z|Y,\theta^{(i)})$$
求期望得：

$$\sum_ZlogP(Y|\theta)P(Z|Y,\theta^{(i)})=\sum_ZlogP(Y,Z|\theta)P(Z|Y,\theta^{(i)})-\sum_ZlogP(Z|Y,\theta)P(Z|Y,\theta^{(i)})$$

等式左边与Z无关，分布求和等于1，于是：

$$
\begin{align}
logP(Y|\theta)&=\sum_ZlogP(Y,Z|\theta)P(Z|Y,\theta^{(i)})-\sum_ZlogP(Z|Y,\theta)P(Z|Y,\theta^{(i)})\\
&=Q(\theta,\theta^{(i)})-H(\theta,\theta^{(i)})
\end{align}
$$

分别取$\theta$为$\theta^{(i)}$和$\theta^{(i+1)}$并相减，有：

$$logP(Y|\theta^{(i+1)})-logP(Y|\theta^{(i)})=[Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)})]-[H(\theta^{(i+1)},\theta^{(i)})-H(\theta^{(i)},\theta^{(i)})]$$

要使收敛性成立，只需上式右端是非负即可，又由于$Q(\theta^{(i+1)})$是$Q(\theta,\theta^{(i)})$达到极大，所以有$Q(\theta^{(i+1)},\theta^{(i)})-Q(\theta^{(i)},\theta^{(i)})≥0$, 要使右端为非负，只需$H(\theta^{(i+1)},\theta^{(i)})-H(\theta^{(i)},\theta^{(i)})≤0$,于是：

$$
\begin{align}
H(\theta^{(i+1)},\theta^{(i)})-H(\theta^{(i)},\theta^{(i)})&=\sum_ZlogP(Z|Y,\theta^{(i+1)})P(Z|Y,\theta^{(i)})-\sum_ZlogP(Z|Y,\theta^{(i)})P(Z|Y,\theta^{(i)})\\
&=\sum_Z\left(log\frac{P(Z|Y,\theta^{(i+1)})}{P(Z|Y,\theta^{(i)})}\right)P(Z|Y,\theta^{(i)})\\
&≤log\left(\sum_Z\frac{P(Z|Y,\theta^{(i+1)})}{P(Z|Y,\theta^{(i)})}P(Z|Y,\theta^{(i)})\right)\\
&=log\left(\sum_ZP(Z|Y,\theta^{(i+1)})\right)=0
\end{align}
$$

即收敛性得证。