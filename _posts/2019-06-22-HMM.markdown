---
layout: post
title:  "隐马尔可夫链"
date:   2019-06-22 00:30
categories: algorithm
tags: algorithm, ML
excerpt: 
mathjax: true
---

* content
{:toc}

### 前言
隐马尔可夫模型（Hidden Markov Model，简称HMM），是可以用来标注问题的统计学习模型，描述由马尔可夫链随机生成观测序列的过程，属于生成式模型。
在正常的马尔可夫模型中，状态对于观测者来说是直接可见的。这样状态的转换概率便是全部的参数。而在隐马尔可夫模型中，状态并不是直接可见的，但是受状态影响的某些变量是可见的。每一个状态在可能输出的符号上都有一个概率分布，因此输出序列能否透露出状态序列的一些信息。

### HMM模型的定义
隐马尔可夫模型由初始概率分布，状态转移概率分布以及观测概率分布确定。
我们假设Q是所有可能的隐藏状态的集合，V是所有可能的观测状态的集合，即：

$$Q={q_1,q_2,...,q_N},V={v_1,v_2,...,v_M}$$

其中，N是可能的状态数，M是可能的观测数。
I是长度为T的状态序列，O是对应的观测序列。

$$I=(i_1,i_2,...,i_T), O=(o_1,o_2,...,o_T)$$

A是状态转移概率矩阵：

$$A=[a_{ij}]_{N*N}$$

其中，

$$a_{ij}=P(i_{t+1}|i_t=q_i),  i=1,2,...,N; j=1,2,...,N$$

表示在时刻t处于状态$q_i$的条件下在时刻t+1转移到状态$q_j$的概率。

B是观测概率矩阵：

$$B=[b_j(k)]_{N*M}$$

其中，

$$b_j(k)=P(o_t=v_k|i_t=q_j), k=1,2,...,M;j=1,2,...,N$$

表示在时刻t处于状态$q_j$的条件下生成观测$v_k$的概率。

$\pi$是初始状态概率向量：

$$\pi=(\pi_i)$$

其中：

$$\pi_i=P(i_1=q_i),i=1,2,...,N$$

表示时刻t=1处于状态$q_i$的概率。

一个马尔可夫模型，可以有隐藏状态的初始概率分布$\pi$，状态转移矩阵A和观测状态转移矩阵B决定，$\pi$，A决定状态序列，B决定观测序列。因此，隐马尔可夫模型$\lambda$可以用三员符号表示，即：

$$\lambda=(A,B,\pi)$$


$\pi$,A,B称为隐马尔可夫模型的三要素。

隐马尔可夫做了两个很重要的基本能假设：

1、齐次马尔可夫假设，即假设隐藏的马尔可夫链在任意时刻t的状态只依赖于前一时刻的状态，于其他时刻的状态及观察无关，也与时刻t无关，即：

$$P(i_t|i_{t-1},o_{t-1},...,i_1,o_1)=P(i_t|i_{t-1})$$

2、观察独立性假设，即假设任意时刻的观测值依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关，即：

$$P(o_t|i_T,o_T,...,i_{t+1},o_{t+1},...,i_1,o_1)=P(o_t|i_t)$$

### HMM的3个经典问题

HMM模型建立起来后，一般我们会讨论3个问题：

1、评估问题：给定模型$\lambda=(A,B,\pi)$和观测序列$O=(o_1,o_2,...,o_T)$，计算在模型$\lambda$下观测序列O出现的概率
$$P(O|\lambda)$$ => （前向算法）

2、预测问题：也称解码，给定观测序列O和模型参数$\lambda$，我们怎样选择一个状态序列，以便能够最好的解释观测序列，即求最有可能的状态序列（条件概率
$$P(I|O)$$最大）。=> (维比特算法)

3、学习问题：给定观测序列O,找到一个能最好的解释观测序列的模型$\lambda=(A,B,\pi)$，即使得
$$P(O|\lambda)$$最大。=> (EM算法，前后向算法)

#### 评估描述

给定观测序列$O=(o_1,o_2,...,o_T)$和模型$\lambda=(A,B,\lambda)$,求观测序列O出现的概率最大的概率$P(O|\lambda)$。解决该问题的最直观的想法是把所有可能的隐藏状态都遍历一遍，得到对应观测状态的概率有多大，这些可能全部加起来就是得到这个观测状态的可能性。
1、全概率公式求解观测序列在所有可能的隐藏状态序列下的概率之和。

$$P(O|\lambda)=\sum_IP(O|I,\lambda)P(I|\lambda)$$

2、任意隐藏状态序列$I=(i_1,i_2,...,i_T)$的概率是

$$P(I|\lambda)=\pi_{i_1}a_{i_1}{i_2}...a_{i_{T-1}}{i_T}$$

3、已知状态序列$\lambda$下，观测序列$O=(o_1,o_2,...,o_T)$的概率

$$P(O|I,\lambda)=b_{i_1}(o_1)b_{i_2}(o_2)...b_{i_T}(o_T)$$

4、O和I同时出现的联合概率为:

$$
\begin{align}
P(O,I|\lambda)&=P(O|I,\lambda)P(I|\lambda)\\

&=\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}(o_T)
\end{align}
$$

5、对所有可能的隐藏状态序列I求和得:

$$
\begin{align}
P(O|\lambda)&=\sum_IP(O|I,\lambda)P(I|\lambda)\\

&=\sum_{i_1i_2...i_T}\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}i_T}b_{i_T}(o_T)
\end{align}
$$

利用上式计算量很大，达到$O(TN^T)$阶，在状态集合和隐藏状态序列比较短时，还是有可能穷举所有可能的状态序列，如果长的话，穷举的数量太大，指数级的，就很难完成。

##### 前向算法

给定HMM模型$\lambda$，定义到时刻t部分观测序列为$o_1,o_2,..o_t$且状态为$q_i$的概率为前向概率，记为：

$$\alpha_t(i)=P(o_1,o_2,...,o_t,i=q_i|\lambda)$$

通过递推求得前向概率$\alpha_t(i)$即观测序列概率
$$P(O|\lambda)$$

算法

输入：隐马尔可夫模型$\lambda$,观测序列O
输出：观测序列概率$P(O|\lambda)$
1、初值

$$\alpha_t(i)=\pi_ib_i(o_i)$$

2、递推，对t=1,2,...,T-1

$$\alpha_t(i)=[\sum_{j=1}^{N}\alpha_t(j)a_{ji}]b_i(o_{t+1})$$

3、终止

$$P(O|\lambda)=\sum_{i=1}^{N}\alpha_T(i)$$


#### 预测描述

##### 近似算法


##### 维比特算法
维比特算法实际是用动态规划解马尔可夫模型的预测问题，即用动态规划求概率最大路径，这时一条路径对应着一个状态序列。

算法

输入：模型$\lambda=(A,B,\pi)$和观测序列$$O=(o_1,o_2,...,o_T)$$

输出：最优路径$$I^{*}=(i_1^*,i_2^*,...,i_T^*)$$

1、初始化：

$$\delta_1(i)=\pi_ib_i(o_1),i=1,2,..,N$$

$$\Psi_1(i)=0,i=1,2,...,N$$

2、递推，对$$t=2,3,...,T$$

$$\delta_t(i)=\max_{1≤j≤N}[\delta_{t-1}(j)a_ji]b_i(o_t),i=1,2,...,N$$

$$\Psi_t(i)=arg\max_{1≤j≤N}[\delta_{t-1}(j)a_ji],i=1,2,...,N$$

3、终止

$$P^*=\max_{1≤i≤N}\delta_T(i)$$

$$i_T^*=arg\max_{1≤i≤N}\delta_T(i)$$

4、最优路径回溯，对$t=T-1,T-2,...,1$

$$i_t^*=\Psi_{t+1}(i_{t+1}^*)$$

求得$$I^{*}=(i_1^*,i_2^*,...,i_T^*)$$

